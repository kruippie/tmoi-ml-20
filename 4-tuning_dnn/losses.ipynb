{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to use which losses?\n",
    "\n",
    "You can find an overview of implemented losses in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a binary classification problem, you will typically have labels True and False, which equals to labels 1 and 0. When using a sigmoid in your output layer, you will get values in the range $[0,1]$ which are interpreted as probailities.\n",
    "\n",
    "The base formula for cross entropy is $-y \\cdot log(p(y))$ where $y$ is the true label, and $p(y)$ is the chance you predicted that your label is 1. The chance will be on the range $[0,1]$.\n",
    "\n",
    "If your label is 1, and you predict a chance of 0.9, you can calculate the binary cross entropy by \n",
    "multiplying the negative label with the log of the chance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.10536051565782628"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 1\n",
    "py = 0.9\n",
    "\n",
    "-y * np.log(py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a label 1, and predicted a low probability of 0.1, we need the loss to be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.3025850929940455"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 1\n",
    "py = 0.1\n",
    "\n",
    "-y * np.log(py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, if the label is 0 and we predicted a low probability of 0.1, that would have been right. We can obtain this by subtracting from 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.10536051565782628"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "py = 0.1\n",
    "\n",
    "-(1-y) * np.log(1-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.302585092994046"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "py = 0.9\n",
    "\n",
    "-(1-y) * np.log(1-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine these two situations in one formula. If the label is 1 or 0, one of the two parts will go to zero and will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N y_i log(p(y_i)) + (1-y_i) log (1-p(y_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow implements this as the Binary Croosentropy loss. With `from_logits` set to false, the predicted value is expected to be on the range $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.23101759>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 0, 1]\n",
    "yhat = [0.1, 0.7, 0.3, 0.9]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we dont use a sigmoid, we can get the output of a linear model, which is called a logit and has a valeu on the range $[-\\infty, \\infty]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.00999736>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0, 1, 0, 1]\n",
    "yhat = [-10.3, 3.2, -17.18, 12.92]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are two or more label classes in a one-hot encoding, you can use categorical cross entropy.\n",
    "\n",
    "Let's say we have three possible classes, and the label is the first class, we will have $[1, 0 ,0]$. If we predict the first class with high probability, but maybe a small chance for the second, we will predict probabilities (and use a generalized version of the sigmoid for multiple classes, which is the softmax. The softmax will make the values sum to zero), so we get something like $[0.95, 0.05,, 0]$ \n",
    "\n",
    "If you have logits on the range $[-\\infty, \\infty]$ as output (cf, you dont use an activation function) you can set `from_logits=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.1769392"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0, 1, 0], [0, 0, 1]]\n",
    "yhat = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y, yhat).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse categorical cross entropy\n",
    "\n",
    "In the case of a lot of classes, a one-hot encoding can be impractical. So we can use a sparse representation, where we can write $[0,1,0]$ as 1, and $[0,0,1]$ as 2. If you have logits on the range $[-\\infty, \\infty]$ as output instead of probabilities(cf, you dont use an sigmoid activation function) you can set `from_logits=True`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.1769392"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 2]\n",
    "yhat = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "scce(y, yhat).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi class, multi label\n",
    "You can also have multiple classes, and more then one label to be right. E.g. you can have a three classes for movies ['commedy', 'sci-fi', 'horror'] and you are watching a movie that is both a comedy, and sci-fi. Your label will be $[1, 0, 1]$ and your prediction might be something like $[0.7, 0.9, 0.1]$. Or in the case of an x-ray of a chest: you might have pneumonia and/or cancer, or none of them.\n",
    "\n",
    "In this case, you should use the binary crossentropy. If you use a softmax, your values will sum to zero. But that is not what you want! Because it is multilabel, you want to allow for multiple values to get close to one, so use a sigmoid as activation function. You can also use this with logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.00449203, 0.00225358], dtype=float32)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[1, 0, 1], [0, 0, 1]]\n",
    "y_pred = [[5.0, -10.0, 5], [-5.0, -10, 20]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default choice for regression is the mean square error.\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y)=\\frac{1}{m}\\sum_{i=1}^m (y-\\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "8.992133333333332"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([10.2, 5.1, 8.12])\n",
    "yhat = np.array([5.2, 6.0, 9.2])\n",
    "loss = tf.keras.losses.MSE(y, yhat)\n",
    "\n",
    "assert np.array_equal(\n",
    "    loss.numpy(), np.mean(np.square(y - yhat), axis=-1))\n",
    "\n",
    "loss.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the square punishes outliers (can you find the outlier in the yhat?), the mean average error puts a smaller penalty on outliers.\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y)=\\frac{1}{m}\\sum_{i=1}^m |y-\\hat{y}|$$\n",
    "\n",
    "Try to change the outlier in the code, and note how the two loss functions react differently to the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.3266666666666667"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([10.2, 5.1, 8.12])\n",
    "yhat = np.array([5.2, 6.0, 9.2])\n",
    "loss = tf.keras.losses.MAE(y, yhat)\n",
    "\n",
    "assert np.array_equal(\n",
    "    loss.numpy(), np.mean(np.abs(y - yhat), axis=-1))\n",
    "\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the target value has a huge spread, you might want to be easier on errors for the very large values. With this, you can use the mean squared logarithmic error:\n",
    "\n",
    "$$\\mathcal{L}(\\hat{y}, y)=\\frac{1}{m}\\sum_{i=1}^m ((log(y+1) -log(\\hat{y} + 1))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.02543662"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1, 10, 1000]\n",
    "yhat = [1.2, 13, 1100]\n",
    "loss = tf.keras.losses.mean_squared_logarithmic_error(y, yhat)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that to a regular mse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3336.3467"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.keras.losses.MSE(y, yhat)\n",
    "loss.numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca2ed108e0829ad954ac36f354f1cc4b518ca95c97ddb3ce5ba4e3a95bf1dea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python38364bittensorflowconda60937b29d62348d0bc10902efa6e00a3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}