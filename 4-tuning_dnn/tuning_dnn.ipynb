{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "#!rm -rf logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/raoulg/tmoi-ml-20/blob/master/4-tuning_dnn/tuning_dnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on google colab, uncomment this line to install keras tuner\n",
    "#!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "We start with the MNIST. We already now from last lesson how easy (or difficult) it is to get a certain performance, so we can evaluate the impact of the different types of activations, batchnorm and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((6000, 28, 28), (6000,))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "We start with a baselinemodel, that is the result from the hypertuning of last lesson. Note how I put the rescaling inside the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "score = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# takes about 1min30s\n",
    "tf.random.set_seed(42) \n",
    "# this random.set_seed makes sure that the random initialization \n",
    "# of the weight is every time the same.\n",
    "# normally, you won't need that, but this makes sure \n",
    "# that the lesson has the exact same output, every time you run it.\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['base'] = model.fit(X_train, y_train, epochs=7, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['base'] = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that normally, you would set the epochs high (eg `epochs=-100`) and let the `EarlyStopping` interrupt the notebook. Because every epoch takes quite some time, I ran it once, and adjusted the runtime by reducing the amount of epochs. This just saves time when I need to rerun the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymax = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "Let's try the SELU. We need the data to be scaled by a standard scaler, and our kernel intializer should be `lecun_normal`. Note that we have to remove the rescaling inside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_trains = scaler.fit_transform(X_train.reshape(len(X_train), -1))\n",
    "X_valids = scaler.transform(X_valid.reshape(len(X_valid), -1))\n",
    "X_tests = scaler.transform(X_test.reshape(len(X_test), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#takes about 1min15s\n",
    "tf.random.set_seed(42)\n",
    "model = Sequential([\n",
    "    Dense(256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    Dense(256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    Dense(256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['selu']  = model.fit(X_trains, y_train, epochs=6, validation_data=(X_valids, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['selu'] = model.evaluate(X_tests, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_train), np.std(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not really an improvement. A problem is that the data is not really normal distributed, with a mean of 33 and a std of 78. Tweaking that might give you better results, but for now we will leave it as it is. Let's try GELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#takes about 1min20s\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow_addons.layers import GELU\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256),\n",
    "    GELU(),\n",
    "    Dense(256),\n",
    "    GELU(),\n",
    "    Dense(256),\n",
    "    GELU(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['gelu']  = model.fit(X_train, y_train, epochs=6, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['gelu'] = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(score, ymin=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a small improvement. However, this is also slightly random. Changing the `random_seed` might change this outcome. Note, we are talking about very small differences, in the order of tenths of a percent. Now let's try `elu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#takes about 1min15s\n",
    "tf.random.set_seed(42)\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['elu'] = model.fit(X_train, y_train, epochs=6, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['elu']=model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a `LeakyReLU`. Note how we have to do that: no activation specified in the `Dense` layer! Note that tweaking the alpha could improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#takes about 1min30s\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "tf.random.set_seed(42)\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['leaky']  = model.fit(X_train, y_train, epochs=7, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['leaky']=model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(score, ymin=0.95, ymax=0.985)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: we can improve slightly on the baseline model. Note that this is sensitive on random things. Initializing the weights differently might give different results. Also note how the baselinemodel is better on the validation set, than the leaky model, but on the testset the leaky model wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchnorm\n",
    "Now, let's see what happens when we add a Batchnorm. First on our baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#takes about 2min30s\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['batchnorm_relu'] = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['batchnorm_relu']=model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an improvement! And, because we had reasonable results with GELU and LeakyReLU, let's try those too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# takes about 2min30s\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow_addons.layers import GELU\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256),\n",
    "    GELU(),\n",
    "    BatchNormalization(),\n",
    "    Dense(256),\n",
    "    GELU(),\n",
    "    BatchNormalization(),\n",
    "    Dense(256),\n",
    "    GELU(),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['batchnorm_gelu']  = model.fit(X_train, y_train, epochs=9, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['batchnorm_gelu'] = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, yscale='linear', subset='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the LeakeReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#takes about 2min30s\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    Dense(256),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['batchnorm_leaky']  = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['batchnorm_leaky'] = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, alpha = 0.2, yscale='linear', subset='batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(score, ymin=0.95, ymax=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have had different results, when running this notebook mutitple times. Often, the batchnorm improves the results. While it might sometimes reduce the result, note that the differences for the MNIST are really small; we are talking about a few last 'hard nuts'. The problems here are dominantly in our odel approach: if a user rotates a number too much, or hasnt centered it, our results get off because we look at every pixel as a feature and don't take context into account. \n",
    "\n",
    "Nevertheless, usually, adding `BatchNormalization` is a good idea.\n",
    "Interestingly, I ended up with `relu` again often. This shows that, even though the function is really simple, it is often a great first pick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Now, let's experiment with Dropout. We start with a single dropout layer, just before the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# takes about 2min37s\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['batchnorm_relu_1dropout'] = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['batchnorm_relu_1dropout']=model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, yscale='linear', subset='batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add dropouts after every layer (except after the last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# takes about 4min total\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "result['batchnorm_relu_3dropout40'] = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['batchnorm_relu_3dropout40']=model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, yscale='linear', subset = 'batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem weird! The train goes worse, but the valid goes better! It's actually the best, so far. Note that it takes a lot more time to train, too.\n",
    "\n",
    "Let's create a forloop to test different values of the dropout, to see what that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.random.set_seed(42)\n",
    "drops = [0.3, 0.2, 0.1]\n",
    "for drop in drops:\n",
    "    name = 'batchnorm_relu_3dropout{0}'.format(int(drop*100))\n",
    "\n",
    "    model = Sequential([\n",
    "        Flatten(),\n",
    "        Rescaling(1./255),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    result[name] = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "    score[name] = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, yscale='linear', subset = 'dropout[0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(result, grid=True, ymax = 0.3, subset='dropout[0-9]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result might suprise you. We can see that the training result is *worse* than the validation result with the dropout. That is something you normally don't expect! How does that make sense? Is the model underfitting, instead of overfitting? \n",
    "\n",
    "Well, actually, no. Or, it might. But it makes sense with dropout.\n",
    "\n",
    "What we are seeing, is the impact of the dropout. Because the dropout drops random units during training, the train result will underperform, but it will give a better result during validation when there are no longer drops active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(score, ymin=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dropout, the batchnorm_relu is improved for rates of 30, 20 and 10%. We might get improvements with leaky or gelu too; try it out if you are curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Convolutions and Hypermodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lesson, we will dive into the theory behind convolutions and maxpooling. For now, let's just try to add these new types of layers to see their effect.\n",
    "\n",
    "- a Conv2D layer expects (as the name suggests) **2D data**, but with an additional dimension for color / grayscale. So, every example has shape (batchsize x height x width) which does not has a channel defined. So we need to add an additional dimension, to specify that we have just one channel for color . We can do this with either reshape, or just with `Input(shape=[28,28,1])`. We will end with a **4D shape** for all data, eg (batchsize x height x width x channels)\n",
    "- We can tune the amount of filters (first argument) and the size of the kernel (second argument). In this example, I have fixated the kernelsize at 3, and set the amount of filters as a hyperparameters to be tuned by the model.\n",
    "- After a convolution, you can add a MaxPool2D layer. Again, **4D shapes** are expected (1D for batch, 2D imagedimensions with 1D for channels).\n",
    "- the amount of combinations of Conv2d and MaxPool2D is something that we will hypertune.\n",
    "- After we are finished with convolutions & pooling, we want to pass the result to our Dense layers. However, they still expect the data to have a shape (batchsize x features), so we need to **flatten the 2D shape** into a 1D feature vector.\n",
    "- For the Dense layers, we will add Batchnormalization and Dropouts. The amount of dropout is a hyperparameter to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.layers import LeakyReLU, Reshape\n",
    "\n",
    "def build_model(hp):\n",
    "    input = Input(shape = [28,28])\n",
    "    x = Rescaling(1./255)(input)\n",
    "    x = Reshape((28,28,1))(x)\n",
    "    \n",
    "    filters = hp.Int('filters', 16, 64, 4)\n",
    "    x = Conv2D(filters, (3,3), activation='relu')(x)\n",
    "    x = MaxPool2D((2,2))(x)\n",
    "    \n",
    "    for i in range(hp.Int('conv_layers', 0, 2)):\n",
    "        x = Conv2D(filters, (3,3), activation='relu')(x)\n",
    "        x = MaxPool2D((2,2))(x)\n",
    "        name = 'convlayer_{0}'.format(i)\n",
    "        \n",
    "    flat = Flatten()(x)\n",
    "\n",
    "    units = hp.Int('units', 128, 320, 64)\n",
    "    drops = hp.Float('drops', 0.1, 0.4)\n",
    "    leak = hp.Float('leak', 0, 0.2)\n",
    "\n",
    "    x = Dense(units)(flat)\n",
    "    x = LeakyReLU(alpha=leak)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(drops)(x)\n",
    "\n",
    "    for i in range(hp.Int('dense_layers', 1, 5)):\n",
    "        name = 'layer_{0}'.format(i)\n",
    "        x = Dense(units=units)(x)\n",
    "        x = LeakyReLU(alpha=leak)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(drops)(x)\n",
    "    \n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs = [input], outputs = [output])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 59s]\n",
      "val_loss: 0.046819593757390976\n",
      "\n",
      "Best val_loss So Far: 0.046819593757390976\n",
      "Total elapsed time: 00h 02m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "# takes about 10 minutes with max_epochs=5 and factor=3\n",
    "# takes about 5 minutes with max_epochs=3 and factor=2\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=2, # increasing this increases the total amount of trials\n",
    "    factor=3, # decreasing this will increase the amount of total trials\n",
    "    seed=10, # adding a random seed here, guarantees you get the same outcome.\n",
    "    hyperband_iterations=1, # this runs the hyperband multiple times, from scratch, when increased.\n",
    "    overwrite=True, # overwrites old runs, so you don't need to remove the folder.\n",
    "    directory='ktuner',\n",
    "    project_name='mnist'\n",
    ")\n",
    "tuner.search(X_train, y_train, validation_data = (X_valid, y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print(best_hps.values)\n",
    "cnn_model = tuner.get_best_models()[0]\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "tf.random.set_seed(42)\n",
    "result['hyper'] = cnn_model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score['hyper'] = cnn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, subset='drop|hyper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(score, ymin=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperParameters\n",
    "hp = HyperParameters()\n",
    "# you can fix some of the values to decrease the size of the search space\n",
    "# lets fixate the units and the conv_layers, because I'm most confident about those values.\n",
    "# This way, the hyperband can focus on exploring the rest\n",
    "hp.Fixed('units', value=256)\n",
    "hp.Fixed('conv_layers', value=1)\n",
    "\n",
    "# takes about 5 minutes with max_epochs=3 and factor=2\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    hyperparameters=hp, # this overwrites the old values with the fixed values\n",
    "    tune_new_entries=True, # this allows the rest that isn't defined to be trained\n",
    "    objective='val_loss',\n",
    "    max_epochs=3, # increasing this increases the total amount of trials\n",
    "    factor=2, # decreasing this will increase the amount of total trials\n",
    "    seed=10, # adding a random seed here, guarantees you get the same outcome.\n",
    "    hyperband_iterations=1, # this runs the hyperband multiple times, from scratch, when increased.\n",
    "    overwrite=True, # overwrites old runs, so you don't need to remove the folder.\n",
    "    directory='ktuner',\n",
    "    project_name='mnist'\n",
    ")\n",
    "tuner.search(X_train, y_train, validation_data = (X_valid, y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print(best_hps.values)\n",
    "model = tuner.get_best_models()[0]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "result['hyper3'] = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "score['hyper3'] = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(result, ymin=0, ymax = 0.15, subset='drop|hyper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(score, ymin=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your model\n",
    "After all this hard work and tuning, save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('mymodel.h5')\n",
    "\n",
    "# Check its architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python38364bittensorflowconda60937b29d62348d0bc10902efa6e00a3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}