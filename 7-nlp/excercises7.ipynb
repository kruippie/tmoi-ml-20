{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXym_zab5n6O"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Input, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data. This are two classes. Every class has it's own file. Our task it to discern clickbait titles from non-clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.6B.100d.txt',\n",
       " 'glove.6B.200d.txt',\n",
       " 'glove.6B.300d.txt',\n",
       " 'glove.6B.50d.txt',\n",
       " 'glove.6B.zip']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = '~/shared'\n",
    "files = !ls $datadir\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getfiles = ['clickbait_data.txt', 'non_clickbait_data.txt']\n",
    "if 'clickbait_data.txt' not in files:\n",
    "    for file in getfiles:\n",
    "        url = \"https://raw.githubusercontent.com/SnehilVerma/Clickbait-Detection/master/{}\".format(file)\n",
    "        req = requests.get(url)\n",
    "        url_content = req.content\n",
    "        csv_file = open(file, 'wb')\n",
    "        csv_file.write(url_content)\n",
    "        csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbnCcr5Y6gkb"
   },
   "outputs": [],
   "source": [
    "file1 = os.path.join(os.path.expanduser(datadir), 'clickbait_data.txt')\n",
    "click = pd.read_csv(file1, header=None, delimiter='\\n', names=['text'])\n",
    "click['label'] = 1\n",
    "\n",
    "file2 = os.path.join(os.path.expanduser(datadir), 'non_clickbait_data.txt')\n",
    "noclick = pd.read_csv(file2, header=None, delimiter='\\n',  names=['text'])\n",
    "noclick['label'] = 0\n",
    "\n",
    "data = pd.concat([click, noclick], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the size of all observations, and define a batchsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(data)\n",
    "BATCH = 32\n",
    "SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `tf.data.Dataset`. You can feed it the `text` and `label` columns as a single tuple, eg `(data['text'], data['label'])`. After that, shuffle the dataset with `buffer_size=SIZE` and make batch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtwRieAi42LQ"
   },
   "outputs": [],
   "source": [
    "ds = \n",
    "ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check one batch visually with `take(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GI0kBWBu46_q",
    "outputId": "d2fbc5f9-c884-411f-e4c6-35c608aa63aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train and test set with a 80% split. Remember that your dataset is batched, so you should use `SIZE/BATCH` as the total amount of items.\n",
    "\n",
    "Use `.take()` and `.skip()` to take the first n observations, and then skip the first n observations to create your sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doVXNEMqARyj"
   },
   "outputs": [],
   "source": [
    "train_n = \n",
    "train_ds = \n",
    "val_ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.prefetch()` with `tf.data.experimental.AUTOTUNE` to prefetch the data. This speeds up performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkUaFOMj5n6T"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = \n",
    "train_ds = \n",
    "val_ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and preprocess the data\n",
    "We can preprocess the text. First, it would make sense to change everything to lowercase with `tf.strings.lower`, and the to replace the punctuation with `tf.strings.regex_replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_s2elK1_BUCw",
    "outputId": "c30b0ffe-3f8b-40d5-d64f-c0d3f5814e0d"
   },
   "outputs": [],
   "source": [
    "punctuation = '[%s]' % string.punctuation\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    x = \n",
    "    x = \n",
    "    return x\n",
    "\n",
    "custom_standardization(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creater a `TextVectorization` layer. Pick a `vocab_size` and `sequence_length`, and add your `custom_standardization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3joqFAu5n6U",
    "outputId": "bfd5c68d-f556-4e41-e6d6-e38d088765f3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pick a vocabulary size and number of words in a sequence.\n",
    "vocab_size = \n",
    "sequence_length =\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=\n",
    "    max_tokens=\n",
    "    output_mode='int',\n",
    "    output_sequence_length= \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `.adapt` to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model with the following architecture:\n",
    "\n",
    "- an input layer with `shape=[1]` and `dtype=tf.string`\n",
    "- your vectorize_layer\n",
    "- an `Embedding` layer. Set the embedding to 100.\n",
    "- `GlobalAveragePooling1D`\n",
    "- one `Dense` layer, with 64 units and `relu`\n",
    "- a final `Dense` layer with one unit and a `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuO810Rj5n6V"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dropout, GlobalAveragePooling1D\n",
    "model = Sequential([\n",
    "\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile it with Adam and a $10^{-4}$ learningrate, with binary_crossentropy as loss. Try to figure out how to add precision and recall to the metrics.\n",
    "Train for 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0gHDX5j5n6V",
    "outputId": "e3600d8d-346c-4db9-b003-4965844e5990"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model.compile(optimizer=,\n",
    "              loss=,\n",
    "              metrics=[])\n",
    "\n",
    "\n",
    "model.fit(train_ds, epochs=3, validation_data=val_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Ui-Z1yDSg-"
   },
   "source": [
    "# Rotten Tomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try something a bit more complex.\n",
    "\n",
    "We download the data, if it is not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cancer_data.csv',\n",
       " 'cancer_data_uncleaned.csv',\n",
       " 'clickbait_data.txt',\n",
       " 'dataset1.csv',\n",
       " 'dataset2.csv',\n",
       " 'non_clickbait_data.txt',\n",
       " 'rotten_tomatoes_movies.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = '../data'\n",
    "files = !ls $datadir\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'rotten_tomatoes_movies.csv'\n",
    "if file not in files:\n",
    "    url = \"https://raw.githubusercontent.com/raoulg/tmoi-ml-20/master/data/rotten_tomatoes_movies.csv\"\n",
    "    req = requests.get(url)\n",
    "    url_content = req.content\n",
    "    path = os.path.join(os.path.expanduser(datadir), file)\n",
    "    csv_file = open(path, 'wb')\n",
    "    csv_file.write(url_content)\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9NFzXYNDUKc"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.expanduser(datadir), file)\n",
    "data = pd.read_csv(path)\n",
    "df = data[['movie_info', 'genres']]\n",
    "df = df.dropna()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a description of the movie as unstructured text and a set of labels.\n",
    "\n",
    "Let's check how many different genres we have. Interesting enough, this is a multilabel dataset, meaning that every move can belong to multiple labels at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "set(flatten([txt.split(\", \") for txt in df.genres.values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That might be a bit too much. Let's start out with just a subset of the labels. We can always increase the amount of labels to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5TSzfGBXu-T"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df['select'] = df.genres.apply(lambda x: re.findall('Science Fiction|Romance|Comedy|Action|Art', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be usefull to create a one-hot encoding. This way, we can generate a model with as a final layer as much units as we have classes. \n",
    "\n",
    "Another option could be to use a \"sparse\" loss function, but let's just try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lsUNl0VaKX_",
    "outputId": "427a8608-0a72-4d7a-b091-f1468df62012"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "X = df['movie_info']\n",
    "y = mlb.fit_transform(df['select'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rshaDM7au5E",
    "outputId": "6c0441d2-af4f-412b-9822-77fcebfd8d16"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to get rid of every observation that has zero labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyTnKc1DaTdb",
    "outputId": "05f17656-74a8-4f86-81b9-af984b08eaea"
   },
   "outputs": [],
   "source": [
    "keep = np.sum(y, axis=1) != 0\n",
    "X = X[keep]\n",
    "y = y[keep]\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we lost about 6000 movies, but we still have enough to make a model. If you want to experiment, you can add more categories and see if you can still get good results. But first, let us visualize the distribution of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "t9MOshXJQVBM",
    "outputId": "28c850c3-cf1c-4ca0-f787-2905bd2b80f6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distribution = np.mean(y, axis=0)\n",
    "# your plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's not a uniform distribution. But we have all categories covered. So while we might want to add precision and recall to be sure, this will probably work. We might get into problems if we had one category really under-represented (eg 0.01%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2poYT2DDjdm",
    "outputId": "5d117c3e-ad38-4199-d02f-52c80dc8e0e2"
   },
   "outputs": [],
   "source": [
    "SIZE = len(X)\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5edYr6DYSlgd",
    "outputId": "e1ce473a-810c-4e7e-f433-ed7986b8f01b"
   },
   "outputs": [],
   "source": [
    "CLASSES = y.shape[1]\n",
    "CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before:\n",
    "- generate datasets from tensor slices\n",
    "- shuffle and batch\n",
    "- pick a train-test ratio\n",
    "- create sets with `take` and `skip`\n",
    "- prefetch with AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayf2p1nsE0wE"
   },
   "outputs": [],
   "source": [
    "ds = \n",
    "ds = \n",
    "\n",
    "train_n = \n",
    "train_ds = \n",
    "val_ds = \n",
    "\n",
    "AUTOTUNE = \n",
    "train_ds = \n",
    "val_ds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOyo5yrYGTrh",
    "outputId": "38d928f1-4762-4dca-ca49-31b4f507c0fa"
   },
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "  print(x)\n",
    "  print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a long review of a move, and multiple genres.\n",
    "\n",
    "First we set up a `TextVectorization` layer. Pick a sensible size for the `max_tokens` and `output_sequence_length`. If you are unsure of a proper size, test the impact of different sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uymEJ7XHmHB"
   },
   "outputs": [],
   "source": [
    "vocab_size = \n",
    "sequence_length = \n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `adapt` to get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWRs5ztLIkSX"
   },
   "outputs": [],
   "source": [
    "text_ds = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a model that has:\n",
    "- InputLayer\n",
    "- vectorizelayer\n",
    "- Embedding of dim 50\n",
    "- GlobalAveragePooling1D\n",
    "- Dense with 64 units and a relu\n",
    "- Dense with amount of classes. Don't use and activation in the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ5i58CzIo9r"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we didn't use an activation in the last layer, we got \"logits\" that range from $[-\\infty, +\\infty]$ instead of values between $[0,1]$ as we would have gotten with a sigmoid activation. Because of this, we have to tell the loss function we need `from_logits` to be `True`.\n",
    "\n",
    "Try to increase and decrease the predictions by modifying the numbers below. First, decide if you want to get the loss up  or down. Then, modify the prediction. Check if you understand whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAHnVc0TKpqu",
    "outputId": "9ab10db1-c145-4221-dce9-ecfe2fb7bda8"
   },
   "outputs": [],
   "source": [
    "y_true = [[1, 0, 1], [0, 0, 1]]\n",
    "y_pred = [[5.0, -10.0, 5], [-5.0, -10, 20]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the function with Adam and binary_crossentropy with logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcBD000-JPXH"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F1hyjMcKVHT",
    "outputId": "e8a52050-6715-4673-9adb-1ddcdfba8912",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the model. We grab the first two texts from our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in val_ds.take(1):\n",
    "    print(text[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(text[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the original label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.inverse_transform(label[:2].numpy())\n",
    "# note that \"Science Fiction\" is actually \"Science Fiction & Fantasy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for ourselves, we can use `inverse_tranform` from the `mlb`. It is interesting how to model actually adds something to the original binary labels. While both examples migth predict correctly a movie to be comedy, the model tells us that it is much more clear from the text that the second one is a comedy (eg with values of 4 versus 12). Also, for a single movie, it can tell you which labels seems to be more likely or dominant. Try for yourself some more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create an architecture with a RNN. use the following:\n",
    "- an `Input` layer\n",
    "- your `vectorize_layer`\n",
    "- an `Embedding` layer\n",
    "- a type of RNN. Try `GRU` first, with 16 units.\n",
    "- A final `Dense` layer, without an activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hpwh3BixLToF",
    "outputId": "b4a6eeb9-bf37-46eb-886c-220d5768fc32"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efVp1D-MNujQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5SSLpYgOzFV",
    "outputId": "10ef28cf-294d-4977-e5f3-c6d82bb39e51"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds,\n",
    "          epochs=20,\n",
    "          validation_data=val_ds,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one is better? \n",
    "What do you hypothesize that is happening?\n",
    "Discuss with other students and the teacher."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "excercises7-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python38364bittensorflowconda60937b29d62348d0bc10902efa6e00a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}