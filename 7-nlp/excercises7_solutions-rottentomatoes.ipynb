{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXym_zab5n6O",
        "outputId": "666fce51-8b94-48f2-b145-6e3e3e577dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Input, MaxPooling1D, InputLayer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "%load_ext tensorboard\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download the data\n",
        "First, we load the data. This are two classes. Every class has it's own file. Our task it to discern clickbait titles from non-clickbait"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cancer_data.csv',\n",
              " 'cancer_data_uncleaned.csv',\n",
              " 'clickbait_data.txt',\n",
              " 'dataset1.csv',\n",
              " 'dataset2.csv',\n",
              " 'non_clickbait_data.txt',\n",
              " 'rotten_tomatoes_movies.csv']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datadir = '../data'\n",
        "files = !ls $datadir\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "getfiles = ['clickbait_data.txt', 'non_clickbait_data.txt']\n",
        "if 'clickbait_data.txt' not in files:\n",
        "    for file in getfiles:\n",
        "        url = \"https://raw.githubusercontent.com/SnehilVerma/Clickbait-Detection/master/{}\".format(file)\n",
        "        req = requests.get(url)\n",
        "        url_content = req.content\n",
        "        path = os.path.join(os.path.expanduser(datadir), file)\n",
        "        csv_file = open(path, 'wb')\n",
        "        csv_file.write(url_content)\n",
        "        csv_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we load in the data, and prepare the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jbnCcr5Y6gkb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32000"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file1 = os.path.join(os.path.expanduser(datadir), 'clickbait_data.txt')\n",
        "click = pd.read_csv(file1, header=None, delimiter='\\n', names=['text'])\n",
        "click['label'] = 1\n",
        "\n",
        "file2 = os.path.join(os.path.expanduser(datadir), 'non_clickbait_data.txt')\n",
        "noclick = pd.read_csv(file2, header=None, delimiter='\\n',  names=['text'])\n",
        "noclick['label'] = 0\n",
        "\n",
        "data = pd.concat([click, noclick], ignore_index=True)\n",
        "\n",
        "SIZE = len(data)\n",
        "BATCH = 32\n",
        "SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Should I Get Bings</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31995</th>\n",
              "      <td>To Make Female Hearts Flutter in Iraq, Throw a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31996</th>\n",
              "      <td>British Liberal Democrat Patsy Calton, 56, die...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31997</th>\n",
              "      <td>Drone smartphone app to help heart attack vict...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31998</th>\n",
              "      <td>Netanyahu Urges Pope Benedict, in Israel, to D...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31999</th>\n",
              "      <td>Computer Makers Prepare to Stake Bigger Claim ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  label\n",
              "0                                     Should I Get Bings      1\n",
              "1          Which TV Female Friend Group Do You Belong In      1\n",
              "2      The New \"Star Wars: The Force Awakens\" Trailer...      1\n",
              "3      This Vine Of New York On \"Celebrity Big Brothe...      1\n",
              "4      A Couple Did A Stunning Photo Shoot With Their...      1\n",
              "...                                                  ...    ...\n",
              "31995  To Make Female Hearts Flutter in Iraq, Throw a...      0\n",
              "31996  British Liberal Democrat Patsy Calton, 56, die...      0\n",
              "31997  Drone smartphone app to help heart attack vict...      0\n",
              "31998  Netanyahu Urges Pope Benedict, in Israel, to D...      0\n",
              "31999  Computer Makers Prepare to Stake Bigger Claim ...      0\n",
              "\n",
              "[32000 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a `tf.data.Dataset`. You can feed it the `text` and `label` columns as a single tuple, eg `(data['text'], data['label'])`. After that, shuffle the dataset with `buffer_size=SIZE` and make batch the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EtwRieAi42LQ"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((data[\"text\"], data['label']))\n",
        "ds = ds.shuffle(buffer_size=SIZE).batch(BATCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a look at the first batch.\n",
        "Check one batch visually with `take(1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI0kBWBu46_q",
        "outputId": "ae82aa21-f577-47f3-ca77-e0fdb7fe927c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'Sinkhole reported in Buffalo, New York'\n",
            " b'Sturm und Drang About Pint-Size Neighbors'\n",
            " b\"23 Times Justin Bieber's Instagram Awakened Your Inner Thirst\"\n",
            " b'23 Things Teachers Actually Want For Christmas'\n",
            " b'These People Go On A Bar Crawl To Find The Best Nachos In LA'\n",
            " b'13 Lies Your Depression Is Telling You'\n",
            " b'Music promoter Mean Fiddler sold for almost \\xc2\\xa338m'\n",
            " b'21 Times Angie Was The Best Part Of \"30 Rock\"'\n",
            " b'Canada to revisit same-sex marriage issue next week'\n",
            " b'People Met Pet Rats For The First Time And Lost Their Damn Minds'\n",
            " b'This Is What Your $10 Ikea Lack Table Looks Like Inside'\n",
            " b'What 10 World Landmarks Could Look Like In An Epic Drought'\n",
            " b'Which Member Of 5 Seconds Of Summer Are You'\n",
            " b'Reading the Brains of Pigeons in Flight'\n",
            " b\"New Yorker's Obama cover sparks outrage\"\n",
            " b'Can You Identify These Meryl Streep Movies By Their Amazon Reviews'\n",
            " b'15 Things That Happen When People Think You Don\\'t \"Look Latina\"'\n",
            " b'32 Of The Greatest Things That Happened On Tumblr In 2015'\n",
            " b'Compact Fluorescent Bulbs Draw Quality Complaints'\n",
            " b'Tensions Grow in Georgia Over Accusations of a Plot'\n",
            " b'19 Life Lessons From \"500 Days Of Summer\"'\n",
            " b'27 Songs Guaranteed To Make You Smile Every Time'\n",
            " b'Would You Rather: The New Years Resolution Edition'\n",
            " b'Ovechkin and Capitals Surge Past Penguins'\n",
            " b'Medical School Says Ex-Army Surgeon, Hid Medtronic Ties'\n",
            " b'Sonakshi Sinha Snapchatted Herself Doodling And Accidentally Drew Kim Kardashian'\n",
            " b'A TV Channel Has Done The Perfect Joke About \"Groundhog Day\"'\n",
            " b\"The results of the Antiquorum's auction of vintage and modern timepieces\"\n",
            " b'20 Things That Will Make Your Breakup Hurt So Much Less'\n",
            " b'10 Awesome Things You Can Make Out Of Duct Tape'\n",
            " b\"'Sahara' opens as top film in U.S., Canada\"\n",
            " b'When an Ear Witness Decides the Case'], shape=(32,), dtype=string)\n",
            "tf.Tensor([0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 0 0], shape=(32,), dtype=int64)\n",
            "(32,)\n",
            "(32,)\n"
          ]
        }
      ],
      "source": [
        "for x, y in ds.take(1):\n",
        "  print(x)\n",
        "  print(y)\n",
        "  print(x.shape)\n",
        "  print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we see a lot of titles, some are clickbait, and some are not. The labels are integers 1 or 0.\n",
        "\n",
        "Now, create a train-valid-test split.\n",
        "\n",
        "Create a train and test set with a 80% split. Remember that your dataset is batched, so you should use `SIZE/BATCH` as the total amount of items.\n",
        "\n",
        "Use `.take()` and `.skip()` to take the first n observations, and then skip the first n observations to create your sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "doVXNEMqARyj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(800, 100, 100)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_n = int(SIZE/BATCH * 0.8)\n",
        "test_n = int(SIZE/BATCH * 0.1)\n",
        "train_ds = ds.take(train_n)\n",
        "valtest = ds.skip(train_n)\n",
        "val_ds = valtest.take(test_n)\n",
        "test_ds = valtest.skip(test_n)\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use `.prefetch()` with `tf.data.experimental.AUTOTUNE` to prefetch the data. This speeds up performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BkUaFOMj5n6T"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhXrkg495n6U"
      },
      "source": [
        "# Clean and preprocess the data\n",
        "We can preprocess the text. First, it would make sense to change everything to lowercase with `tf.strings.lower`, and the to replace the punctuation with `tf.strings.regex_replace`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s2elK1_BUCw",
        "outputId": "04bc4a44-21df-4990-d653-d5555fb63dfb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "array([b'sinkhole reported in buffalo new york',\n",
              "       b'sturm und drang about pintsize neighbors',\n",
              "       b'23 times justin biebers instagram awakened your inner thirst',\n",
              "       b'23 things teachers actually want for christmas',\n",
              "       b'these people go on a bar crawl to find the best nachos in la',\n",
              "       b'13 lies your depression is telling you',\n",
              "       b'music promoter mean fiddler sold for almost \\xc2\\xa338m',\n",
              "       b'21 times angie was the best part of 30 rock',\n",
              "       b'canada to revisit samesex marriage issue next week',\n",
              "       b'people met pet rats for the first time and lost their damn minds',\n",
              "       b'this is what your 10 ikea lack table looks like inside',\n",
              "       b'what 10 world landmarks could look like in an epic drought',\n",
              "       b'which member of 5 seconds of summer are you',\n",
              "       b'reading the brains of pigeons in flight',\n",
              "       b'new yorkers obama cover sparks outrage',\n",
              "       b'can you identify these meryl streep movies by their amazon reviews',\n",
              "       b'15 things that happen when people think you dont look latina',\n",
              "       b'32 of the greatest things that happened on tumblr in 2015',\n",
              "       b'compact fluorescent bulbs draw quality complaints',\n",
              "       b'tensions grow in georgia over accusations of a plot',\n",
              "       b'19 life lessons from 500 days of summer',\n",
              "       b'27 songs guaranteed to make you smile every time',\n",
              "       b'would you rather the new years resolution edition',\n",
              "       b'ovechkin and capitals surge past penguins',\n",
              "       b'medical school says exarmy surgeon hid medtronic ties',\n",
              "       b'sonakshi sinha snapchatted herself doodling and accidentally drew kim kardashian',\n",
              "       b'a tv channel has done the perfect joke about groundhog day',\n",
              "       b'the results of the antiquorums auction of vintage and modern timepieces',\n",
              "       b'20 things that will make your breakup hurt so much less',\n",
              "       b'10 awesome things you can make out of duct tape',\n",
              "       b'sahara opens as top film in us canada',\n",
              "       b'when an ear witness decides the case'], dtype=object)>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "punctuation = '[%s]' % string.punctuation\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  x = tf.strings.lower(input_data) # all tekst to lowercase\n",
        "  x = tf.strings.regex_replace(x, punctuation, '')\n",
        "  return x\n",
        "\n",
        "custom_standardization(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creater a `TextVectorization` layer. Pick a `vocab_size` and `sequence_length`, and add your `custom_standardization`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3joqFAu5n6U",
        "outputId": "61ce8370-9071-4726-8f21-0bfba0f35c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 755 ms, sys: 77.1 ms, total: 832 ms\n",
            "Wall time: 493 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Pick a vocabulary size and number of words in a sequence.\n",
        "vocab_size = 5000\n",
        "sequence_length = 20\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to \n",
        "# integers. Note that the layer uses the custom standardization defined above. \n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "#text_ds = train_ds.map(lambda x, y: x)\n",
        "text_ds = train_ds.map(tf.autograph.experimental.do_not_convert(lambda x, y: x))\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a model with the following architecture:\n",
        "\n",
        "- an input layer with `shape=[1]` and `dtype=tf.string`\n",
        "- your vectorize_layer\n",
        "- an `Embedding` layer. Set the embedding to 100.\n",
        "- `GlobalAveragePooling1D`\n",
        "- one `Dense` layer, with 64 units and `relu`\n",
        "- a final `Dense` layer with one unit and a `sigmoid`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GuO810Rj5n6V"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, Dropout, GlobalMaxPooling1D\n",
        "model = Sequential([\n",
        "    InputLayer(input_shape=[1], dtype=tf.string),\n",
        "    vectorize_layer,\n",
        "    Embedding(vocab_size, 100),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile it with Adam and a $10^{-4}$ learningrate, with binary_crossentropy as loss. Try to figure out how to add precision and recall to the metrics.\n",
        "Train for 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0gHDX5j5n6V",
        "outputId": "e3600d8d-346c-4db9-b003-4965844e5990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.5764 - accuracy: 0.8808 - precision: 0.8515 - recall: 0.9230 - val_loss: 0.3427 - val_accuracy: 0.9422 - val_precision: 0.9613 - val_recall: 0.9190\n",
            "Epoch 2/3\n",
            "800/800 [==============================] - 2s 3ms/step - loss: 0.2064 - accuracy: 0.9509 - precision: 0.9614 - recall: 0.9394 - val_loss: 0.1297 - val_accuracy: 0.9631 - val_precision: 0.9683 - val_recall: 0.9559\n",
            "Epoch 3/3\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1068 - accuracy: 0.9671 - precision: 0.9711 - recall: 0.9630 - val_loss: 0.0869 - val_accuracy: 0.9722 - val_precision: 0.9719 - val_recall: 0.9725\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd008c61b80>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "\n",
        "model.fit(train_ds, epochs=3, validation_data=val_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 0.9744 - precision: 0.9720 - recall: 0.9769\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.08805856108665466,\n",
              " 0.9743750095367432,\n",
              " 0.9720149040222168,\n",
              " 0.9768750071525574]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Ui-Z1yDSg-"
      },
      "source": [
        "# Rotten Tomatoes\n",
        "Now, let's try something a bit more complex.\n",
        "\n",
        "We download the data, if it is not present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cancer_data.csv',\n",
              " 'cancer_data_uncleaned.csv',\n",
              " 'clickbait_data.txt',\n",
              " 'dataset1.csv',\n",
              " 'dataset2.csv',\n",
              " 'non_clickbait_data.txt',\n",
              " 'rotten_tomatoes_movies.csv']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datadir = '../data'\n",
        "files = !ls $datadir\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = 'rotten_tomatoes_movies.csv'\n",
        "if file not in files:\n",
        "    url = \"https://raw.githubusercontent.com/raoulg/tmoi-ml-20/master/data/rotten_tomatoes_movies.csv\"\n",
        "    req = requests.get(url)\n",
        "    url_content = req.content\n",
        "    path = os.path.join(os.path.expanduser(datadir), file)\n",
        "    csv_file = open(path, 'wb')\n",
        "    csv_file.write(url_content)\n",
        "    csv_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And load it to memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C9NFzXYNDUKc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_info</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Always trouble-prone, the life of teenager Per...</td>\n",
              "      <td>Action &amp; Adventure, Comedy, Drama, Science Fic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kate (Catherine Keener) and her husband Alex (...</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A successful, middle-aged Hollywood songwriter...</td>\n",
              "      <td>Comedy, Romance</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          movie_info  \\\n",
              "0  Always trouble-prone, the life of teenager Per...   \n",
              "1  Kate (Catherine Keener) and her husband Alex (...   \n",
              "2  A successful, middle-aged Hollywood songwriter...   \n",
              "\n",
              "                                              genres  \n",
              "0  Action & Adventure, Comedy, Drama, Science Fic...  \n",
              "1                                             Comedy  \n",
              "2                                    Comedy, Romance  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = os.path.join(os.path.expanduser(datadir), file)\n",
        "data = pd.read_csv(path)\n",
        "df = data[['movie_info', 'genres']]\n",
        "df = df.dropna()\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a description of the movie as unstructured text and a set of labels.\n",
        "\n",
        "Let's check how many different genres we have. Interesting enough, this is a multilabel dataset, meaning that every move can belong to multiple labels at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Action & Adventure',\n",
              " 'Animation',\n",
              " 'Anime & Manga',\n",
              " 'Art House & International',\n",
              " 'Classics',\n",
              " 'Comedy',\n",
              " 'Cult Movies',\n",
              " 'Documentary',\n",
              " 'Drama',\n",
              " 'Faith & Spirituality',\n",
              " 'Gay & Lesbian',\n",
              " 'Horror',\n",
              " 'Kids & Family',\n",
              " 'Musical & Performing Arts',\n",
              " 'Mystery & Suspense',\n",
              " 'Romance',\n",
              " 'Science Fiction & Fantasy',\n",
              " 'Special Interest',\n",
              " 'Sports & Fitness',\n",
              " 'Television',\n",
              " 'Western'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flatten = lambda t: [item for sublist in t for item in sublist]\n",
        "set(flatten([txt.split(\", \") for txt in df.genres.values]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That might be a bit too much. Let's start out with just a subset of the labels. We can always increase the amount of labels to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z5TSzfGBXu-T"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "df['select'] = df.genres.apply(lambda x: re.findall('Science Fiction|Romance|Comedy|Action|Art', x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be usefull to create a one-hot encoding. This way, we can generate a model with as a final layer as much units as we have classes. \n",
        "\n",
        "Another option could be to use a \"sparse\" loss function, but let's just try this out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lsUNl0VaKX_",
        "outputId": "427a8608-0a72-4d7a-b091-f1468df62012"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17372, 5)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "X = df['movie_info']\n",
        "y = mlb.fit_transform(df['select'])\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rshaDM7au5E",
        "outputId": "6c0441d2-af4f-412b-9822-77fcebfd8d16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 1],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0],\n",
              "       ...,\n",
              "       [1, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1Y2AvdR1awbc",
        "outputId": "1820dcda-91d2-4bd0-d8f6-4142ca2d365c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_info</th>\n",
              "      <th>genres</th>\n",
              "      <th>select</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Always trouble-prone, the life of teenager Per...</td>\n",
              "      <td>Action &amp; Adventure, Comedy, Drama, Science Fic...</td>\n",
              "      <td>[Action, Comedy, Science Fiction]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kate (Catherine Keener) and her husband Alex (...</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>[Comedy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A successful, middle-aged Hollywood songwriter...</td>\n",
              "      <td>Comedy, Romance</td>\n",
              "      <td>[Comedy, Romance]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Following the closing arguments in a murder tr...</td>\n",
              "      <td>Classics, Drama</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In 1866, Professor Pierre M. Aronnax (Paul Luk...</td>\n",
              "      <td>Action &amp; Adventure, Drama, Kids &amp; Family</td>\n",
              "      <td>[Action]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          movie_info  \\\n",
              "0  Always trouble-prone, the life of teenager Per...   \n",
              "1  Kate (Catherine Keener) and her husband Alex (...   \n",
              "2  A successful, middle-aged Hollywood songwriter...   \n",
              "3  Following the closing arguments in a murder tr...   \n",
              "4  In 1866, Professor Pierre M. Aronnax (Paul Luk...   \n",
              "\n",
              "                                              genres  \\\n",
              "0  Action & Adventure, Comedy, Drama, Science Fic...   \n",
              "1                                             Comedy   \n",
              "2                                    Comedy, Romance   \n",
              "3                                    Classics, Drama   \n",
              "4           Action & Adventure, Drama, Kids & Family   \n",
              "\n",
              "                              select  \n",
              "0  [Action, Comedy, Science Fiction]  \n",
              "1                           [Comedy]  \n",
              "2                  [Comedy, Romance]  \n",
              "3                                 []  \n",
              "4                           [Action]  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we want to get rid of every observation that has zero labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyTnKc1DaTdb",
        "outputId": "05f17656-74a8-4f86-81b9-af984b08eaea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11368, 11368)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keep = np.sum(y, axis=1) != 0\n",
        "X = X[keep]\n",
        "y = y[keep]\n",
        "len(X), len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omkiJFd_bGFU",
        "outputId": "d292741f-ac79-4aef-efa2-8d575ddab39b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[1, 0, 1, 0, 1],\n",
              "        [0, 0, 1, 0, 0],\n",
              "        [0, 0, 1, 1, 0],\n",
              "        ...,\n",
              "        [1, 0, 1, 0, 0],\n",
              "        [1, 1, 0, 0, 0],\n",
              "        [1, 1, 0, 0, 0]]),\n",
              " (11368,),\n",
              " (11368, 5))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y, X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2poYT2DDjdm",
        "outputId": "5d117c3e-ad38-4199-d02f-52c80dc8e0e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11368"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SIZE = len(X)\n",
        "BATCH = 32\n",
        "SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we lost about 6000 movies, but we still have enough to make a model. If you want to experiment, you can add more categories and see if you can still get good results. But first, let us visualize the distribution of the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "t9MOshXJQVBM",
        "outputId": "28c850c3-cf1c-4ca0-f787-2905bd2b80f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 5 artists>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "distribution = np.mean(y, axis=0)\n",
        "plt.bar(range(len(distribution)), distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5edYr6DYSlgd",
        "outputId": "e1ce473a-810c-4e7e-f433-ed7986b8f01b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CLASSES = y.shape[1]\n",
        "CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same as before:\n",
        "- generate datasets from tensor slices\n",
        "- shuffle and batch\n",
        "- pick a train-test ratio\n",
        "- create sets with `take` and `skip`\n",
        "- prefetch with AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ayf2p1nsE0wE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(284, 37, 35)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "ds = ds.shuffle(buffer_size=SIZE).batch(BATCH)\n",
        "\n",
        "train_n = int(SIZE/BATCH * 0.8)\n",
        "test_n = int(SIZE/BATCH * 0.1)\n",
        "\n",
        "train_ds = ds.take(train_n)\n",
        "testvalid = ds.skip(train_n)\n",
        "test_ds = testvalid.take(test_n)\n",
        "val_ds = testvalid.skip(test_n)\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's peak at the first batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOyo5yrYGTrh",
        "outputId": "38d928f1-4762-4dca-ca49-31b4f507c0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'Commuting to Manhattan on the same train, two married strangers (Robert De Niro, Meryl Streep) meet by accident and have an affair.'\n",
            " b'Each week Kable (Gerard Butler), a death-row inmate, battles his fellow prisoners in a violent online game called \"Slayers,\" his every move controlled by a young gamer\\'s remote device. To the players, Kable and the other inmates are just simulated characters. But, to a resistance group that opposes the game\\'s inventor, Kable is a critical component of their plan to end the inventor\\'s form of high-tech slavery.'\n",
            " b\"Katie Feldman (Julianne Hough) moves to a small town on the North Carolina coast, determined to make a new life for herself. She takes a job as a waitress and keeps a low profile, but she is soon won over by the warmth and caring of the close-knit community, especially that of widower Alex (Josh Duhamel). With the help of Alex and his children, Katie learns to love and trust again -- but when a mysterious stranger arrives and starts asking questions, Katie's dark past threatens to reclaim her.\"\n",
            " b'Klaatu (Keanu Reeves), an extraterrestrial visitor to planet Earth, becomes the herald of upheaval on a global scale. As the world\\'s governments and scientists race to understand what is happening and how to stop it, Dr. Helen Benson (Jennifer Connelly) and her stepson come to understand the chilling ramifications behind Klaatu\\'s statement that he is a \"friend to the Earth.\"'\n",
            " b\"Gang leader Cody Jarrett (James Cagney) lives for his mother, planning heists between horrible headaches. During a train robbery that goes wrong, Cody shoots an investigator. Realizing Cody will never be stopped if he knows he's being pursued, authorities plant undercover agent Hank (Edmond O'Brien) in Cody's cell. When his mother dies, a distraught Cody breaks out of jail, bringing Hank along to join his gang. With Hank in communication with the police, Cody plans a payroll heist.\"\n",
            " b'Coming in on the tail end of a rhythm and blues singing group explosion, The Five Heartbeats (Robert Townsend, Michael Wright, Leon, Harry J. Lennix, Tico Wells) rise and fall within the space of seven years. Along the way, the group deals with all manner of fame and fortune distractions -- jealousy, greed, too much womanizing and drugs all take a toll. Their troubles culminate when executive Big Red (Hawthorne James) is arrested for the murder of manager Jimmy Potter (Chuck Patterson).'\n",
            " b'In the land of Prydain, lowly pig herder Taran (Grant Bardsley) dreams of becoming a gallant knight. Young Taran receives his heroic calling when the evil Horned King (John Hurt) kidnaps Hen-Wren, a prophesying pig that had been entrusted to Taran. Now, with help from his furry sidekick Gurgi and Princess Eilonwy, Taran must locate the magical black cauldron before the Horned King is able to use its mystical powers to summon an army of the undead.'\n",
            " b\"In this documentary, filmmaker Nick Broomfield follows the saga of Aileen Wuornos, a prostitute who has been accused of committing a brutal series of murders. Broomfield conducts interviews with Wuornos herself, and his crew films her trial as well as her interactions with religious fanatic Arlene Pralle, who gives Wuornos dubious advice and legally adopts her. The cameras also roll as the accused's attorney ignores the case at hand to negotiate a deal to sell his client's story.\"\n",
            " b\"Gambler Nathan Detroit (Frank Sinatra) has few options for the location of his big craps game. Needing $1,000 to pay a garage owner to host the game, Nathan bets Sky Masterson (Marlon Brando) that Sky cannot get virtuous Sarah Brown (Jean Simmons) out on a date. Despite some resistance, Sky negotiates a date with her in exchange for bringing people into her mission. Meanwhile, Nathan's longtime fianc\\xc3\\xa9e, Adelaide (Vivian Blaine), wants him to go legit and marry her.\"\n",
            " b'Doug and his pal, Skeeter, befriend a creature created by toxins that were dumped into Lucky Duck Lake.'\n",
            " b'Alex Owens (Jennifer Beals) is a beautiful young woman who works a day job in a steel mill and dances in a bar at night. When Alex discovers that her handsome boss, Nick Hurley (Michael Nouri), is both interested in her and supportive of her performing career, she renews her efforts to get accepted into a prestigious dance conservatory. Although Alex is frightened of failure, she is cheered on by Nick, as well as by her mentor, former ballet performer Hanna Long (Lilia Skala).'\n",
            " b'Rogue San Francisco cop \"Dirty Harry\" Callahan (Clint Eastwood) doesn\\'t believe in blind obedience to the rules, but when a vigilante group starts executing criminals who have escaped punishment on technicalities, that\\'s going too far. Against the orders of his commanding officer, Lt. Briggs (Hal Holbrook), Callahan begins investigating his hunch that another policeman is behind the killings -- with his primary suspect being his tightly wound ex-partner, Charlie McCoy (Mitchell Ryan).'\n",
            " b\"A dark tale of infidelity and murder, crime and punishment. Set in a small northern California town of the late 40s, the film portrays Ed Crane (Billy Bob Thornton), a barber dissatisfied with his life. His wife Doris' (Frances McDormand) infidelity presents Ed with an opportunity for blackmail that he thinks will help him to change it. However, Ed's scheme unravels and lays bare even darker secrets...\"\n",
            " b'Enrolled in an educational summer camp, the Stinkers, five 7-year-olds who wreak havoc on their uptight headmaster, Morgan Brinway (B.D. Wong), decide to abduct a sea lion from the local aquarium with the goal of returning it to the sea. They are in competition, however, with the villainous Broccoli (Sam McMurray), who is out to steal the sea lion, teach it a few tricks and sell it to a Bulgarian circus. The kids do all they can to stop the resilient villain and save the friendly beast.'\n",
            " b'A poor English boy is taken in by a wealthy family where he develops an intense relationship with his younger foster sister.'\n",
            " b'With his son (Bj\\xc3\\xb6rn Almroth) in another room, a man (Thorsten Flinck) shoots an amateur porno film in his Swedish flat.'\n",
            " b\"The eight Egyptian musicians who comprise the Alexandria Ceremonial Police Orchestra arrive by mistake in a small town in Israel's Negev Desert. Their booking set for a different city, and with no transportation out of the town or any hotels to stay at, the band settles at a restaurant owned by Dina (Ronit Elkabetz), who offers them lodging. Overcoming ethnic barriers, the Egyptians find diversion and companionship with the Israelis through a pervading undercurrent of shared melancholy.\"\n",
            " b'While on a tropical vacation with her father (G\\xc3\\xa9rard Depardieu), a teen (Katherine Heigl) passes him off as her lover, to impress a boy (Dalton James).'\n",
            " b'Turk Henry, a bassist for a recently split mega-platinum band, takes his pampered wife on an exotic luxury holiday to Chile only for her to be snatched by a group of shipless buccaneers trying to raise money to buy a boat. Despite having never organized anything more challenging than a club sandwich from room service, Turk embarks on a rescue mission, taking him from the back alleys of Santiago to the jungles of South America.'\n",
            " b'A 12-year-old boy develops a telepathic connection with his dog after a middle-school science experiment goes awry.'\n",
            " b\"Distraught following his wife's suicide, American hotelier Paul (Marlon Brando) becomes transfixed by the beautiful younger Frenchwoman Jeanne (Maria Schneider) when he meets her by chance at an apartment both are attempting to rent. The couple begin an extended but purely anonymous sexual relationship in which they do not even tell each other their names, but it soon becomes clear that the couple's deliberate level of disassociation cannot continue.\"\n",
            " b'In 1959 Indochina, Den-Dhin-Chan labor camp is run by Vietnamese warlords and European war criminals. Here, at the worst prison in the land, former boxing champion Martin Tilman has made a name for himself fighting tournaments on which wealthy criminals gamble in high-stakes events. Tilman is due for release, but the corrupt forces running the jail will do everything in their power to keep him locked down.'\n",
            " b'A young man (Luke Wilson) discovers that the beautiful but manipulative woman (Uma Thurman) he is dating is really a superheroine known as G-Girl. After breaking up with her, he learns that \"hell hath no fury like a woman scorned\" as she unleashes her considerable powers against him.'\n",
            " b'Henry Carter (Kevin Spacey), a jaded therapist-to-the-stars, uses marijuana to forget his troubles. His first pro bono case puts him in touch with Jemma (Saffron Burrows), a troubled teen who lives far from the glamour of the Hollywood Hills. Confronted with her real problems, Henry wonders, given his current state of mind, if he can help Jemma find answers to her questions.'\n",
            " b\"During the 1940s, Jay (Mike Damus) and Arty (Brad Stoll) find their lives turned upside down when their father moves south for work, leaving them with their no-nonsense grandmother (Irene Worth) and their loving but absent-minded spinster aunt, Bella (Mercedes Ruehl). Along the way, they're also introduced to Uncle Louie (Richard Dreyfuss), a small-time crook. Underlying tension between Bella and her mother surface when Bella decides to marry her boyfriend, Johnny (David Strathairn).\"\n",
            " b\"Value Shop assistant manager Maya Vargas wants only one thing for her 43rd birthday -- a promotion. While her r\\xc3\\xa9sum\\xc3\\xa9 may not scream upper management, her track record certainly does. Vargas is an innovator who listens to her customers and delivers results. When she loses the job to a college-educated candidate, Maya sets out to prove to Madison Avenue that street smarts are as valuable as book smarts -- and it's never too late for a second act.\"\n",
            " b'During World War II, high school student Homer Macauley (Mickey Rooney) and his boss, Willie Grogan (Frank Morgan), pull their weight at the local telegraph office. Homer is dedicated and enthusiastic at first, but becomes troubled when the office begins receiving death notices about local soldiers. Luckily, he finds inspiration in a new hire, the confident Tom Spangler (James Craig). But Tom has troubles of his own. Smitten with Diana Steed (Marsha Hunt), he too must overcome his insecurities.'\n",
            " b'In 2009, Conan O\\'Brien took over for Jay Leno as host of NBC\\'s \"The Tonight Show,\" but, after only seven months on the job, he suddenly found himself unemployed. Embroiled in a legal battle with the network, Conan took his talents on the road for a 32-city comedy show aptly titled \"The Legally Prohibited from Being Funny on Television Tour.\" Filmmaker Rodman Flender was there every step of the way to document the comic\\'s personal struggles during the most tumultuous time of his career.'\n",
            " b\"Computer programmer Angela Bennett (Sandra Bullock) starts a new freelance gig and, strangely, all her colleagues start dying. Does it have something to do with the mysterious disc she was given? Her suspicions are raised when, during a trip to Mexico, she's seduced by a handsome stranger (Jeremy Northam) intent on locating the same disc. Soon Angela is tangled up in a far-reaching conspiracy that leads to her identity being erased. Can she stop the same thing from happening to her life?\"\n",
            " b\"Lovable Englishman Charles (Hugh Grant) and his group of friends seem to be unlucky in love. When Charles meets a beautiful American named Carrie (Andie MacDowell) at a wedding, he thinks his luck may have changed. But, after one magical night, Carrie returns to the States, ending what might have been. As Charles and Carrie's paths continue to cross -- over a handful of nuptials and one funeral -- he comes to believe they are meant to be together, even if their timing always seems to be off.\"\n",
            " b'Comedian Alvy Singer (Woody Allen) examines the rise and fall of his relationship with struggling nightclub singer Annie Hall (Diane Keaton). Speaking directly to the audience in front of a bare background, Singer reflects briefly on his childhood and his early adult years before settling in to tell the story of how he and Annie met, fell in love, and struggled with the obstacles of modern romance, mixing surreal fantasy sequences with small moments of emotional drama.'\n",
            " b'By kidnapping their women, a tyrant forces the greatest warrior in the world to fight his closest friend.'], shape=(32,), dtype=string)\n",
            "tf.Tensor(\n",
            "[[0 0 0 1 0]\n",
            " [1 0 0 0 1]\n",
            " [0 0 0 1 0]\n",
            " [0 0 0 0 1]\n",
            " [1 0 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [1 0 0 0 1]\n",
            " [0 1 0 0 0]\n",
            " [0 1 1 0 0]\n",
            " [1 0 0 0 0]\n",
            " [0 0 0 1 0]\n",
            " [1 0 0 0 0]\n",
            " [0 0 1 1 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 0 1 0]\n",
            " [0 1 0 0 0]\n",
            " [0 1 1 0 0]\n",
            " [0 0 1 0 0]\n",
            " [1 0 0 0 0]\n",
            " [0 0 1 0 0]\n",
            " [0 1 0 1 0]\n",
            " [1 0 0 0 0]\n",
            " [0 0 1 0 1]\n",
            " [0 0 1 0 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 1 1 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 1 0 0]\n",
            " [1 0 0 0 0]\n",
            " [0 0 1 1 0]\n",
            " [0 0 1 1 0]\n",
            " [1 1 0 0 0]], shape=(32, 5), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_ds.take(1):\n",
        "  print(x)\n",
        "  print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we have a long review of a move, and multiple genres.\n",
        "\n",
        "First we set up a `TextVectorization` layer. Pick a sensible size for the `max_tokens` and `output_sequence_length`. If you are unsure of a proper size, test the impact of different sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3uymEJ7XHmHB"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "sequence_length = 50\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to \n",
        "# integers. Note that the layer uses the custom standardization defined above. \n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length,\n",
        "    name='vectorization_layer')\n",
        "\n",
        "\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make a model that has:\n",
        "- InputLayer\n",
        "- vectorizelayer\n",
        "- Embedding of dim 50\n",
        "- GlobalAveragePooling1D\n",
        "- Dense with 64 units and a relu\n",
        "- Dense with amount of classes. Don't use and activation in the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rJ5i58CzIo9r"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    InputLayer(input_shape=[1], dtype=tf.string),\n",
        "    vectorize_layer,\n",
        "    Embedding(vocab_size, 50),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(CLASSES)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vectorization_layer (TextVec (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 50, 50)            500000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                3264      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 503,589\n",
            "Trainable params: 503,589\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because we didn't use an activation in the last layer, we got \"logits\" that range from $[-\\infty, +\\infty]$ instead of values between $[0,1]$ as we would have gotten with a sigmoid activation. Because of this, we have to tell the loss function we need `from_logits` to be `True`.\n",
        "\n",
        "Try to increase and decrease the predictions by modifying the numbers below. First, decide if you want to get the loss up  or down. Then, modify the prediction. Check if you understand whats happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAHnVc0TKpqu",
        "outputId": "9ab10db1-c145-4221-dce9-ecfe2fb7bda8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.00449203, 0.00225358], dtype=float32)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true = [[1, 0, 1], [0, 0, 1]]\n",
        "y_pred = [[5.0, -10.0, 5], [-5.0, -10, 20]]\n",
        "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
        "loss.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile the function with Adam and binary_crossentropy with logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jcBD000-JPXH"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train for 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F1hyjMcKVHT",
        "outputId": "e8a52050-6715-4673-9adb-1ddcdfba8912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "284/284 [==============================] - 2s 4ms/step - loss: 0.5784 - accuracy: 0.3341 - val_loss: 0.5475 - val_accuracy: 0.3698\n",
            "Epoch 2/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.5176 - accuracy: 0.4364 - val_loss: 0.4564 - val_accuracy: 0.5560\n",
            "Epoch 3/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.4342 - accuracy: 0.6038 - val_loss: 0.3901 - val_accuracy: 0.6621\n",
            "Epoch 4/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.3771 - accuracy: 0.6708 - val_loss: 0.3330 - val_accuracy: 0.7026\n",
            "Epoch 5/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.3290 - accuracy: 0.6992 - val_loss: 0.2919 - val_accuracy: 0.7362\n",
            "Epoch 6/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.2887 - accuracy: 0.7245 - val_loss: 0.2455 - val_accuracy: 0.7595\n",
            "Epoch 7/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.2568 - accuracy: 0.7391 - val_loss: 0.2068 - val_accuracy: 0.7871\n",
            "Epoch 8/20\n",
            "284/284 [==============================] - 2s 5ms/step - loss: 0.2215 - accuracy: 0.7586 - val_loss: 0.1812 - val_accuracy: 0.7879\n",
            "Epoch 9/20\n",
            "284/284 [==============================] - 2s 5ms/step - loss: 0.1965 - accuracy: 0.7718 - val_loss: 0.1754 - val_accuracy: 0.7690\n",
            "Epoch 10/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.1749 - accuracy: 0.7814 - val_loss: 0.1385 - val_accuracy: 0.8190\n",
            "Epoch 11/20\n",
            "284/284 [==============================] - 2s 6ms/step - loss: 0.1524 - accuracy: 0.7898 - val_loss: 0.1312 - val_accuracy: 0.8112\n",
            "Epoch 12/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.1334 - accuracy: 0.7975 - val_loss: 0.1154 - val_accuracy: 0.8138\n",
            "Epoch 13/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.1145 - accuracy: 0.8039 - val_loss: 0.0903 - val_accuracy: 0.8172\n",
            "Epoch 14/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.1004 - accuracy: 0.8095 - val_loss: 0.0797 - val_accuracy: 0.8052\n",
            "Epoch 15/20\n",
            "284/284 [==============================] - 1s 5ms/step - loss: 0.0863 - accuracy: 0.8097 - val_loss: 0.0742 - val_accuracy: 0.8138\n",
            "Epoch 16/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.0735 - accuracy: 0.8119 - val_loss: 0.0590 - val_accuracy: 0.8164\n",
            "Epoch 17/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.0633 - accuracy: 0.8133 - val_loss: 0.0479 - val_accuracy: 0.8276\n",
            "Epoch 18/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.0534 - accuracy: 0.8166 - val_loss: 0.0413 - val_accuracy: 0.8078\n",
            "Epoch 19/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.0445 - accuracy: 0.8192 - val_loss: 0.0356 - val_accuracy: 0.8181\n",
            "Epoch 20/20\n",
            "284/284 [==============================] - 1s 4ms/step - loss: 0.0389 - accuracy: 0.8204 - val_loss: 0.0307 - val_accuracy: 0.8302\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd00bfce460>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logdir = os.path.join(\"logs\", \"emb50_dense64\")\n",
        "tb = TensorBoard(logdir, histogram_freq=0)\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds, \n",
        "    epochs=20,\n",
        "    callbacks=[tb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"For two years, security guard Ben (Kevin Hart) has tried to convince James (Ice Cube), a veteran cop, that he is worthy of James' sister, Angela. When Ben is finally accepted into the police academy, James decides to test his mettle by inviting him along on a shift deliberately designed to scare the trainee. However, events take an unexpected turn when their wild night leads to Atlanta's most-notorious criminal and Ben's rapid-fire mouth proves as dangerous as the bullets whizzing by them.\"\n",
            " b'Seven-year-old cousins Emmeline (Elva Josephson) and Richard (Glenn Kohan) survive a shipwreck and find themselves marooned on a beautiful island somewhere in the Pacific Ocean. Under the tutelage of a fellow castaway, Paddy Button (Leo McKern), the two learn the basics of survival. When Paddy dies, however, the now adolescent Emmeline (Brooke Shields) and Richard (Christopher Atkins) are on their own to discover sex, love and loneliness in a tropical paradise.'], shape=(2,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for text, label in val_ds.take(1):\n",
        "    print(text[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the model. We grab the first two texts from our validation dataset.\n",
        "\n",
        "Use the model to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  3.2538111, -27.1035   ,  15.773536 ,  -9.315605 , -13.173566 ],\n",
              "       [  9.481662 ,  -4.579263 , -11.013162 , -14.259049 ,  -5.070095 ]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(text[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the original label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
              "array([[1, 0, 1, 0, 0],\n",
              "       [1, 0, 0, 0, 0]])>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Action', 'Comedy'), ('Action',)]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlb.inverse_transform(label[:2].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check for ourselves, we can use `inverse_tranform` from the `mlb`. It is interesting how to model actually adds something to the original binary labels. While both examples migth predict correctly a movie to be comedy, the model tells us that it is much more clear from the text that the second one is a comedy (eg with values of 4 versus 12). Also, for a single movie, it can tell you which labels seems to be more likely or dominant. Try for yourself some more examples.\n",
        "\n",
        "Now, create an architecture with a RNN. use the following:\n",
        "- an `Input` layer\n",
        "- your `vectorize_layer`\n",
        "- an `Embedding` layer\n",
        "- a type of RNN. Try `GRU` first, with 16 units.\n",
        "- A final `Dense` layer, without an activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpwh3BixLToF",
        "outputId": "b4a6eeb9-bf37-46eb-886c-220d5768fc32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vectorization_layer (TextVec (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 50, 50)            500000    \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 16)                3264      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 85        \n",
            "=================================================================\n",
            "Total params: 503,349\n",
            "Trainable params: 503,349\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "\n",
        "model = Sequential([\n",
        "    InputLayer(input_shape = [1], dtype=tf.string),\n",
        "    vectorize_layer,\n",
        "    Embedding(vocab_size, 50),\n",
        "    GRU(16),\n",
        "    Dense(CLASSES)\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "efVp1D-MNujQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5SSLpYgOzFV",
        "outputId": "10ef28cf-294d-4977-e5f3-c6d82bb39e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "284/284 [==============================] - 9s 25ms/step - loss: 0.5776 - accuracy: 0.3326 - val_loss: 0.5577 - val_accuracy: 0.3534\n",
            "Epoch 2/20\n",
            "284/284 [==============================] - 6s 21ms/step - loss: 0.5493 - accuracy: 0.3434 - val_loss: 0.5373 - val_accuracy: 0.3362\n",
            "Epoch 3/20\n",
            "284/284 [==============================] - 5s 19ms/step - loss: 0.5104 - accuracy: 0.4198 - val_loss: 0.4474 - val_accuracy: 0.5112\n",
            "Epoch 4/20\n",
            "284/284 [==============================] - 5s 18ms/step - loss: 0.4404 - accuracy: 0.5393 - val_loss: 0.3865 - val_accuracy: 0.6172\n",
            "Epoch 5/20\n",
            "284/284 [==============================] - 5s 19ms/step - loss: 0.3916 - accuracy: 0.6001 - val_loss: 0.3594 - val_accuracy: 0.6414\n",
            "Epoch 6/20\n",
            "284/284 [==============================] - 5s 19ms/step - loss: 0.3541 - accuracy: 0.6455 - val_loss: 0.3246 - val_accuracy: 0.6526\n",
            "Epoch 7/20\n",
            "284/284 [==============================] - 6s 20ms/step - loss: 0.3160 - accuracy: 0.6781 - val_loss: 0.2929 - val_accuracy: 0.6853\n",
            "Epoch 8/20\n",
            "284/284 [==============================] - 5s 19ms/step - loss: 0.2840 - accuracy: 0.7099 - val_loss: 0.2450 - val_accuracy: 0.7319\n",
            "Epoch 9/20\n",
            "284/284 [==============================] - 6s 22ms/step - loss: 0.2523 - accuracy: 0.7289 - val_loss: 0.2175 - val_accuracy: 0.7328\n",
            "Epoch 10/20\n",
            "284/284 [==============================] - 5s 17ms/step - loss: 0.2253 - accuracy: 0.7424 - val_loss: 0.1905 - val_accuracy: 0.7621\n",
            "Epoch 11/20\n",
            "284/284 [==============================] - 5s 18ms/step - loss: 0.1979 - accuracy: 0.7507 - val_loss: 0.1736 - val_accuracy: 0.7724\n",
            "Epoch 12/20\n",
            "284/284 [==============================] - 6s 19ms/step - loss: 0.1749 - accuracy: 0.7619 - val_loss: 0.1484 - val_accuracy: 0.7681\n",
            "Epoch 13/20\n",
            "284/284 [==============================] - 8s 28ms/step - loss: 0.1538 - accuracy: 0.7666 - val_loss: 0.1326 - val_accuracy: 0.7853\n",
            "Epoch 14/20\n",
            "284/284 [==============================] - 7s 24ms/step - loss: 0.1333 - accuracy: 0.7753 - val_loss: 0.1140 - val_accuracy: 0.7836\n",
            "Epoch 15/20\n",
            "284/284 [==============================] - 5s 18ms/step - loss: 0.1207 - accuracy: 0.7790 - val_loss: 0.0944 - val_accuracy: 0.7491\n",
            "Epoch 16/20\n",
            "284/284 [==============================] - 5s 18ms/step - loss: 0.1047 - accuracy: 0.7744 - val_loss: 0.0969 - val_accuracy: 0.7793\n",
            "Epoch 17/20\n",
            "284/284 [==============================] - 5s 19ms/step - loss: 0.0919 - accuracy: 0.7798 - val_loss: 0.0774 - val_accuracy: 0.7466\n",
            "Epoch 18/20\n",
            "284/284 [==============================] - 6s 20ms/step - loss: 0.0824 - accuracy: 0.7734 - val_loss: 0.0717 - val_accuracy: 0.7966\n",
            "Epoch 19/20\n",
            "284/284 [==============================] - 5s 19ms/step - loss: 0.0736 - accuracy: 0.7751 - val_loss: 0.0551 - val_accuracy: 0.7716\n",
            "Epoch 20/20\n",
            "284/284 [==============================] - 6s 20ms/step - loss: 0.0635 - accuracy: 0.7760 - val_loss: 0.0485 - val_accuracy: 0.7767\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd00ce65ca0>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logdir = os.path.join(\"logs\", \"emb50_GRU\")\n",
        "tb = TensorBoard(logdir, histogram_freq=0)\n",
        "model.fit(train_ds,\n",
        "          epochs=20,\n",
        "          validation_data=val_ds,\n",
        "          verbose=1,\n",
        "          callbacks=[tb])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "excercises7-solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
      "name": "python38364bittensorflowconda60937b29d62348d0bc10902efa6e00a3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}