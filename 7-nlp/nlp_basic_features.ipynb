{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a dummy corpus of four sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'please please take care of my cat.',\n",
    "    'my document is the second document.',\n",
    "    'and this document is the third one.',\n",
    "    'have you taken care of my cat?',\n",
    "]\n",
    "labels = [0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `CountVectorizer` we can make a document-term matrix. We get four vectors, where every `document` (which is just one sentence in our dummy example) gets a vector assigned to it with the length of the vocubulary found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'care', 'cat', 'document', 'have', 'is', 'my', 'of', 'one', 'please', 'second', 'take', 'taken', 'the', 'third', 'this', 'you']\n",
      "[[0 1 1 0 0 0 1 1 0 2 0 1 0 0 0 0 0]\n",
      " [0 0 0 2 0 1 1 0 0 0 1 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0]\n",
      " [0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could make that a binary count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0]\n",
      " [1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0]\n",
      " [0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or an n-gram count. In this case, we use ngrams of size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'care of', 'document is', 'have you', 'is the', 'my cat', 'my document', 'of my', 'please please', 'please take', 'second document', 'take care', 'taken care', 'the second', 'the third', 'third one', 'this document', 'you taken']\n",
      "[[0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can change the default `analyzer=\"word\"` to `analyzer=\"char\"` for a characterlevel n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ca', ' do', ' is', ' my', ' of', ' on', ' pl', ' se', ' ta', ' th', ' yo', 'ake', 'and', 'are', 'ase', 'at.', 'at?', 'ave', 'car', 'cat', 'con', 'cum', 'd d', 'd o', 'd t', 'doc', 'e c', 'e o', 'e p', 'e s', 'e t', 'e y', 'eas', 'eco', 'en ', 'ent', 'f m', 'hav', 'he ', 'hir', 'his', 'ird', 'is ', 'ke ', 'ken', 'lea', 'men', 'my ', 'n c', 'nd ', 'ne.', 'nt ', 'nt.', 'ocu', 'of ', 'ond', 'one', 'ou ', 'ple', 'rd ', 're ', 's d', 's t', 'se ', 'sec', 't i', 'tak', 'the', 'thi', 'u t', 'ume', 've ', 'y c', 'y d', 'you']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 75)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can do all sorts of things. For example, we can calculate the distance between every document and every other document. In this case, we use manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 63., 63., 28.],\n",
       "       [63.,  0., 34., 59.],\n",
       "       [63., 34.,  0., 61.],\n",
       "       [28., 59., 61.,  0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "X = X.todense()\n",
    "distance = manhattan_distances(X, X)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a square matrix. For the first entry, you can see that the last document is closer (distance 28) to this document, and the second and third document are more distant with 63 and 63.\n",
    "\n",
    "We can reduce this to 2 dimensions, which is easier for plotting, with something like PCA. Another, often used, way to decompose is tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "model = pca.fit_transform(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize this and add some labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p = pd.DataFrame(model, columns=['x1', 'x2'])\n",
    "p['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fdf30b47310>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data=p, x='x1', y='x2', c='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is essentially an unsupervised model. We only add the labels later on. This can be interpreted as: dots closer to each other are more similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is the tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'care', 'cat', 'document', 'have', 'is', 'my', 'of', 'one', 'please', 'second', 'take', 'taken', 'the', 'third', 'this', 'you']\n",
      "(4, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29236164]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.34336615]]\n",
      "[[0.        ]\n",
      " [0.69571213]\n",
      " [0.32555709]\n",
      " [0.        ]]\n",
      "[[0.23669194]\n",
      " [0.28161946]\n",
      " [0.        ]\n",
      " [0.27798449]]\n"
     ]
    }
   ],
   "source": [
    "names =vectorizer.get_feature_names()\n",
    "\n",
    "k = names.index(\"cat\")\n",
    "print(X.todense()[:,k])\n",
    "\n",
    "k = names.index(\"document\")\n",
    "print(X.todense()[:,k])\n",
    "\n",
    "k = names.index(\"my\")\n",
    "print(X.todense()[:,k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the simple counting, it just weighs the counts.\n",
    "\n",
    "All methods (ngrams, wordcount, tfidf) can also be used for supervised learning. Eg with a simple linear SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [0, 2]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf = LinearSVC()\n",
    "clf.fit(X, labels)\n",
    "yhat = clf.predict(X)\n",
    "\n",
    "print(clf.score(X, labels))\n",
    "confusion_matrix(labels, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing \n",
    "We can lemmatize the text. This could make it easier to find relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'please please take good care of my cat.',\n",
    "    'my document is the second document.',\n",
    "    'and this document was the third one.',\n",
    "    'were you taking better care of my cat?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\" \".join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "please please take good care of my cat. my document is the second document. and this document was the third one. were you taking better care of my cat?"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = []\n",
    "for token in doc:\n",
    "    lemma_list.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'and',\n",
       " 'be',\n",
       " 'care',\n",
       " 'cat',\n",
       " 'document',\n",
       " 'good',\n",
       " 'my',\n",
       " 'of',\n",
       " 'one',\n",
       " 'please',\n",
       " 'second',\n",
       " 'take',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'well',\n",
       " 'you'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(lemma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how all the words \"is\", \"was\" and \"were\" have been reduced to \"be\". Also, \"better\" is reduced to \"good\".\n",
    "\n",
    "## Stopwords\n",
    "In some cases, you might want to filter out all the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence =[] \n",
    "for word in lemma_list:\n",
    "\tlexeme = nlp.vocab[word]\n",
    "\tif lexeme.is_stop == False:\n",
    "\t\tfiltered_sentence.append(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'care',\n",
       " 'cat',\n",
       " '.',\n",
       " 'document',\n",
       " 'second',\n",
       " 'document',\n",
       " '.',\n",
       " 'document',\n",
       " '.',\n",
       " 'care',\n",
       " 'cat',\n",
       " '?']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca2ed108e0829ad954ac36f354f1cc4b518ca95c97ddb3ce5ba4e3a95bf1dea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}